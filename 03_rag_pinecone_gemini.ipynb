{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title-cell",
      "metadata": {},
      "source": [
        "# RAG: Embeddings locales (Transformers) + Pinecone + Gemini \n",
        "\n",
        "Mejores prácticas: chunking con solapamiento, MMR, citas y orquestación LCEL."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "env-note",
      "metadata": {},
      "source": [
        "Crea `.env` en  la ruta del `RAG` con:\n",
        "\n",
        "- `GOOGLE_API_KEY=...` (obligatorio, para respuestas con Gemini)\n",
        "- `PINECONE_API_KEY=...` (obligatorio, para índices/vector store)\n",
        "- `PINECONE_REGION=us-east-1` (opcional, por defecto `us-east-1`)\n",
        "- `PC_INDEX_NAME=...` (obligatorio, usar minúsculas y `-`)\n",
        "- `PC_NAMESPACE=...` (obligatorio, usar minúsculas y `-`)\n",
        "- `HF_EMBED_MODEL=intfloat/multilingual-e5-base` (opcional, embeddings locales)\n",
        "- `PDF_PATH=...` (obligatorio, ruta al PDF)\n",
        "- `CHUNK_SIZE=1000`, `CHUNK_OVERLAP=150` (opcional)\n",
        "- `PC_RESET_IF_DIM_MISMATCH=false` (opcional: `true` para recrear índice si la dimensión no coincide)\n",
        "\n",
        "Nota: No se usan alternativas; si faltan claves obligatorias, se detiene con mensaje claro. Ademas comparti el `.env.example` para referencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pip-install-cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q -U \"langchain>=0.2.12\" langchain-community \"langchain-google-genai>=3.0.0\" langchain-text-splitters \"pinecone-client>=3.0.0\" langchain-pinecone python-dotenv pypdf transformers sentence-transformers \"huggingface_hub[hf_xet]\" langchain_huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44eda757",
      "metadata": {},
      "source": [
        "## BLOQUE 1 — Configuración y entorno\n",
        "- Importa `os`, `re` y `dotenv.load_dotenv` para cargar variables desde `.env`.\n",
        "- Define parámetros clave: `GOOGLE_API_KEY`, `PINECONE_API_KEY`, `HF_EMBED_MODEL`, `INDEX_NAME`, `NAMESPACE`, `PDF_PATH`, `CHUNK_SIZE`, `CHUNK_OVERLAP`.\n",
        "- Realiza `assert` sobre claves obligatorias para fallar temprano si faltan.\n",
        "- Motivo: centraliza configuración y asegura que el entorno tenga credenciales y parámetros antes de continuar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-env-cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "PINECONE_REGION = os.getenv(\"PINECONE_REGION\", \"us-east-1\")\n",
        "HF_EMBED_MODEL = os.getenv(\"HF_EMBED_MODEL\", \"intfloat/multilingual-e5-base\")\n",
        "INDEX_NAME = os.getenv(\"PC_INDEX_NAME\", \" \")\n",
        "NAMESPACE = os.getenv(\"PC_NAMESPACE\", \" \")\n",
        "PDF_PATH = os.getenv(\"PDF_PATH\", r\" \")\n",
        "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"1000\"))\n",
        "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"150\"))\n",
        "PC_RESET_IF_DIM_MISMATCH = os.getenv(\"PC_RESET_IF_DIM_MISMATCH\", \"false\").lower() == \"true\"\n",
        "\n",
        "assert GOOGLE_API_KEY, \"Falta GOOGLE_API_KEY en .env (Gemini es obligatorio).\"\n",
        "assert PINECONE_API_KEY, \"Falta PINECONE_API_KEY en .env (Pinecone es obligatorio).\"\n",
        "\n",
        "print(\"GOOGLE_API_KEY: OK\")\n",
        "print(\"PINECONE_API_KEY: OK\")\n",
        "print(\"PINECONE_REGION:\", PINECONE_REGION)\n",
        "print(\"HF_EMBED_MODEL:\", HF_EMBED_MODEL)\n",
        "print(\"INDEX_NAME:\", INDEX_NAME, \"NAMESPACE:\", NAMESPACE)\n",
        "print(\"PDF_PATH:\", PDF_PATH)\n",
        "print(\"CHUNK_SIZE:\", CHUNK_SIZE, \"CHUNK_OVERLAP:\", CHUNK_OVERLAP)\n",
        "print(\"PC_RESET_IF_DIM_MISMATCH:\", PC_RESET_IF_DIM_MISMATCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "embeddings-title",
      "metadata": {},
      "source": [
        "## Embeddings locales (Transformers / Sentence-Transformers)\n",
        "- Importa `HuggingFaceEmbeddings` desde `langchain_huggingface`.\n",
        "- Crea `doc_emb` con el modelo indicado y normalización de embeddings.\n",
        "- Calcula `detected_dim` (dimensión del vector), útil para configurar el índice de Pinecone.\n",
        "- Motivo: obtener representaciones vectoriales consistentes para consultas y documentos (base del RAG)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "embeddings-cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "doc_emb = HuggingFaceEmbeddings(\n",
        "    model_name=HF_EMBED_MODEL,\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "detected_dim = len(doc_emb.embed_query(\"prueba dimension\"))\n",
        "print(f\"Embeddings locales cargados: {HF_EMBED_MODEL} (dim={detected_dim})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ingesta-title",
      "metadata": {},
      "source": [
        "## Ingesta de PDF y chunking con solapamiento (mejores prácticas)\n",
        "- Importa `PdfReader` (`pypdf`) y `RecursiveCharacterTextSplitter`.\n",
        "- Extrae texto por página, crea metadatos (`source`, `page`) y realiza chunking con solapamiento.\n",
        "- Produce `docs` (lista de `Document` con contenido y metadatos).\n",
        "- Motivo: preparar el corpus en fragmentos manejables que capturen contexto y mejoren la recuperación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ingesta-cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pypdf import PdfReader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "reader = PdfReader(PDF_PATH)\n",
        "pages = [page.extract_text() or \"\" for page in reader.pages]\n",
        "texts, metas = [], []\n",
        "for i, t in enumerate(pages, start=1):\n",
        "    t = (t or \"\").strip()\n",
        "    if not t:\n",
        "        continue\n",
        "    texts.append(t)\n",
        "    metas.append({\"source\": os.path.basename(PDF_PATH), \"page\": i})\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "docs = splitter.create_documents(texts, metadatas=metas)\n",
        "print(f\"Total de chunks: {len(docs)} (chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pinecone-title",
      "metadata": {},
      "source": [
        "## Inicializar Pinecone e índice (sanitización y validación de dimensión)\n",
        "- Importa `Pinecone` (y `ServerlessSpec`), normaliza el nombre del índice y lo crea/valida.\n",
        "- Alinea la dimensión del índice con `detected_dim` y muestra el namespace en uso.\n",
        "- Motivo: disponer de un almacén vectorial escalable para búsquedas semánticas con embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pinecone-cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "def sanitize_index_name(name: str) -> str:\n",
        "    s = name.lower()\n",
        "    s = re.sub(r'[^a-z0-9-]', '-', s)\n",
        "    s = re.sub(r'-{2,}', '-', s)\n",
        "    s = s.strip('-')\n",
        "    return s or 'index'\n",
        "\n",
        "EMBED_DIM = detected_dim\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "safe_index = sanitize_index_name(INDEX_NAME)\n",
        "if safe_index != INDEX_NAME:\n",
        "    print(f\"Renombrando índice inválido '{INDEX_NAME}' → '{safe_index}'\")\n",
        "    INDEX_NAME = safe_index\n",
        "\n",
        "existing = [x.name for x in pc.list_indexes()]\n",
        "if INDEX_NAME not in existing:\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBED_DIM,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=PINECONE_REGION),\n",
        "    )\n",
        "    print(f\"Índice creado: {INDEX_NAME} (dim={EMBED_DIM}, región={PINECONE_REGION})\")\n",
        "else:\n",
        "    desc = pc.describe_index(INDEX_NAME)\n",
        "    idx_dim = getattr(desc, \"dimension\", EMBED_DIM)\n",
        "    if idx_dim != EMBED_DIM:\n",
        "        print(f\"Advertencia: dimensión del índice ({idx_dim}) != embeddings ({EMBED_DIM}).\")\n",
        "        if PC_RESET_IF_DIM_MISMATCH:\n",
        "            print(\"Recreando índice para alinear dimensión...\")\n",
        "            pc.delete_index(INDEX_NAME)\n",
        "            pc.create_index(\n",
        "                name=INDEX_NAME,\n",
        "                dimension=EMBED_DIM,\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec(cloud=\"aws\", region=PINECONE_REGION),\n",
        "            )\n",
        "            print(f\"Índice recreado: {INDEX_NAME} (dim={EMBED_DIM})\")\n",
        "\n",
        "index = pc.Index(INDEX_NAME)\n",
        "print(f\"Usando índice Pinecone: {INDEX_NAME} (namespace='{NAMESPACE}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "upsert-title",
      "metadata": {},
      "source": [
        "## Upsert de chunks en Pinecone (metadatos e ids estables)\n",
        "- Importa `PineconeVectorStore` y construye listas de `texts`, `metadatas` e `ids`.\n",
        "- Crea `vectorstore` y realiza `add_texts` para subir los chunks al índice.\n",
        "- Motivo: persistir los vectores de los documentos en Pinecone como base del retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "upsert-cell",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upsert en Pinecone completado: 101 chunks\n"
          ]
        }
      ],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "texts = [d.page_content for d in docs]\n",
        "metadatas = [d.metadata for d in docs]\n",
        "ids = [f\"{m['page']}-{i}\" for i, m in enumerate(metadatas)]\n",
        "vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=doc_emb, namespace=NAMESPACE)\n",
        "_ = vectorstore.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
        "print(f\"Upsert en Pinecone completado: {len(texts)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "retriever-title",
      "metadata": {},
      "source": [
        "## Retriever MMR y compresión manual basada en embeddings\n",
        "- Importa `numpy`, crea `retriever` con `MMR` para diversidad de resultados.\n",
        "- Define `_cosine` y la clase `SimpleCompressionRetriever` (reranking por similitud coseno: consulta vs documentos).\n",
        "- Crea `cretriever` con `similarity_threshold` y `top_k`, e imprime la configuración activa.\n",
        "- Motivo: recuperar candidatos con MMR y filtrar/reranquear manualmente para maximizar relevancia del contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "retriever-cell",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compresión contextual (manual) habilitada: umbral=0.65, top_k=6\n"
          ]
        }
      ],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "import numpy as np\n",
        "\n",
        "if 'vectorstore' not in globals():\n",
        "    vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=doc_emb, namespace=NAMESPACE)\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 24, \"fetch_k\": 64, \"lambda_mult\": 0.4},\n",
        ")\n",
        "\n",
        "def _cosine(a, b):\n",
        "    a = np.array(a, dtype=np.float32)\n",
        "    b = np.array(b, dtype=np.float32)\n",
        "    na = np.linalg.norm(a)\n",
        "    nb = np.linalg.norm(b)\n",
        "    if na == 0.0 or nb == 0.0:\n",
        "        return 0.0\n",
        "    return float(np.dot(a, b) / (na * nb))\n",
        "\n",
        "class SimpleCompressionRetriever:\n",
        "    def __init__(self, base_retriever, embeddings, similarity_threshold=0.65, top_k=6):\n",
        "        self.base_retriever = base_retriever\n",
        "        self.embeddings = embeddings\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def get_relevant_documents(self, query):\n",
        "        docs = (\n",
        "            self.base_retriever.get_relevant_documents(query)\n",
        "            if hasattr(self.base_retriever, \"get_relevant_documents\")\n",
        "            else self.base_retriever.invoke(query)\n",
        "        )\n",
        "        qv = self.embeddings.embed_query(query)\n",
        "        dvs = self.embeddings.embed_documents([d.page_content for d in docs])\n",
        "        scored = []\n",
        "        for d, v in zip(docs, dvs):\n",
        "            s = _cosine(v, qv)\n",
        "            scored.append((s, d))\n",
        "        filtered = [d for s, d in sorted(scored, key=lambda x: x[0], reverse=True) if s >= self.similarity_threshold]\n",
        "        if not filtered:\n",
        "            filtered = [d for _, d in sorted(scored, key=lambda x: x[0], reverse=True)[: self.top_k]]\n",
        "        return filtered[: self.top_k]\n",
        "\n",
        "    def invoke(self, query):\n",
        "        return self.get_relevant_documents(query)\n",
        "\n",
        "cretriever = SimpleCompressionRetriever(retriever, doc_emb, similarity_threshold=0.65, top_k=6)\n",
        "print(f\"Compresión contextual (manual) habilitada: umbral={cretriever.similarity_threshold}, top_k={cretriever.top_k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rag-title",
      "metadata": {},
      "source": [
        "## Cadena RAG (LCEL) con Gemini y citas\n",
        "- Importa `ChatGoogleGenerativeAI`, `ChatPromptTemplate`, `StrOutputParser`, `RunnablePassthrough`, `RunnableLambda`.\n",
        "- Define `format_docs` para construir el contexto con citas `[p. N]` y el `SYSTEM_PROMPT` con instrucciones claras.\n",
        "- Crea `prompt`, `llm` (`gemini-2.0-flash`, `temperature=0.2`) y el grafo LCEL: `{context, question} -> prompt -> llm -> parser`.\n",
        "- Motivo: orquestar el flujo RAG de forma declarativa con LCEL, garantizando formato y trazabilidad del contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "rag-cell",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cadena RAG con Gemini lista.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "def format_docs(docs):\n",
        "    parts = []\n",
        "    for d in docs:\n",
        "        pg = d.metadata.get(\"page\")\n",
        "        src = d.metadata.get(\"source\")\n",
        "        parts.append(f\"[p. {pg}] ({src})\\n{d.page_content}\")\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"Eres un asistente experto en recuperación de información. Responde en español, claro y estructurado.\\n\"\n",
        "    \"Usa la información del 'Contexto' para contestar con precisión.\\n\"\n",
        "    \"Incluye citas de página en formato [p. N] cuando corresponda. Si falta información, indícalo.\"\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", SYSTEM_PROMPT),\n",
        "    (\"human\", \"Pregunta: {question}\\n\\nContexto:\\n{context}\"),\n",
        "])\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)\n",
        "\n",
        "retrieval = RunnableLambda(lambda q: cretriever.invoke(q))\n",
        "format_ctx = RunnableLambda(format_docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": (retrieval | format_ctx), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "print(\"Cadena RAG con Gemini lista.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "query-title",
      "metadata": {},
      "source": [
        "## Consulta de ejemplo con citas\n",
        "- Define `question`, ejecuta `rag_chain.invoke(question)` y muestra la respuesta.\n",
        "- Recupera `docs_used` desde `cretriever` y lista las páginas citadas.\n",
        "- Motivo: demostrar el flujo completo, verificar páginas usadas y evaluar la calidad de la recuperación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "query-cell",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La primera página del documento indica que la conclusión se encuentra en la página 14 [p. 1].\n",
            "\n",
            "La página 2 explica que la mayoría de lo que se discute bajo el tema de la IA se refiere al aumento de la automatización de tareas mediante el uso del aprendizaje automático y la toma de decisiones automatizada. En el centro de los debates actuales sobre la IA y las aplicaciones de aprendizaje automático se encuentra el uso de algoritmos, que son reglas seguidas por una computadora, programadas por humanos, que traducen los datos de entrada en salidas [p. 2].\n",
            "Citas (páginas) usadas: [1.0, 2.0, 17.0, 18.0, 20.0]\n"
          ]
        }
      ],
      "source": [
        "question = (\n",
        "    \"que dicen las primeras paginas?\"\n",
        ")\n",
        "\n",
        "answer = rag_chain.invoke(question)\n",
        "print(answer)\n",
        "\n",
        "docs_used = cretriever.invoke(question)\n",
        "pages = sorted({d.metadata.get(\"page\") for d in docs_used if d.metadata.get(\"page\")})\n",
        "print(\"Citas (páginas) usadas:\", pages)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
